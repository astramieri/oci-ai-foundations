# Deep Learning Models

Deep Learning Model Architectures:

- **Feed Forward Neural Networks (FNNs)**:  they are the simplest form of neural networks, also called as Multi Layer Perceptron (MLP).

- **Convolutional Neural Networks (CNNs)**: they can automatically detect and learn local patterns and features in images and videos.

- **Recurrent Neural Network (RNNs)**: they are designed to handle sequential data, such as time series data or natural language. They have a feedback loop that allows them to maintain hidden states and capture temporal dependencies.

- **Autoencoders**: they are unsupervised learning models used for feature extraction and dimensionality reduction, and is commonly employed in data compression and anomaly detection.

- **Long Short Term Memory (LSTM)**: it is a specialized RNN variant designed to handle long term dependencies in sequential data.

- **Generative Adversarial Network (GAN)**: it is used for generating realistic synthetic data, such as images, audio, and text.

- **Transformers**: they are widely used in natural language processing and have become state of the art models for tasks, like machine translation, text generation, and language understanding.
